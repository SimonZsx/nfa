\section{Introduction}

The recent paradigm of Network Function Virtualization (NFV) advocates moving
Network Functions (NFs) out of dedicated hardware middleboxes and running them as
virtualized applications on commodity servers \cite{nfv-white-paper}. With NFV, network
operators no longer need to maintain complicated and costly hardware middleboxes. Instead,
they can launch virtualized devices (virtual machines or containers) to run NFs on the fly, which
drastically reduces the cost and complexity of deploying network services, usually consisting of a sequence of NFs such as ``firewall$\rightarrow$IDS$\rightarrow$proxy''.

 %However, simply running NF software in virtualized environment is not enough to satisfy the stringent requirement of NFV. What network operators need is a full-fledged NFV system, that is capable of handling different kinds of NFV management tasks.
A number of NFV management systems have been designed in recent years, \eg, E2 \cite{palkar2015e2}, OpenBox \cite{bremler2015openbox}, CoMb
\cite{sekar2012design}, xOMB \cite{anderson2012xomb}, Stratos
\cite{gember2012stratos}, OpenNetVM \cite{hwang2015netvm, zhang2016opennetvm}, ClickOS \cite{martins2014clickos}. They implement a
broad range of NF management functionalities, including line-speed packet processing, dynamic NF placement, elastic NF scaling,
load balancing, etc., to facilitate network operators in deploying NFs in virtualized environments. However, none of these NF management systems are guaranteed to
be resilient, and they may not enable fault
tolerance \cite{rajagopalan2013pico, sherry2015rollback} and flow
migration \cite{gember2015opennf, rajagopalan2013split, khalid2016paving} simultaneously. %However, not being resilient has serious impact on the applicability of NFV systems.

Failure resilience with flow migration capability is of pivotal importance in practical NFV systems. Existing NF management systems mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, which is in fact only feasible in cases of short-lived flows. In real-world Internet systems, long-lived flows are common. Web applications %such as websites and on-line games
 usually multiplex application-level requests and responses in
one TCP connection to improve performance. For example, a web browser commonly enables HTTP
keep-alive to use one TCP connection to exchange many requests and responses with a
web server \cite{http-keep-alive}. In video-streaming
\cite{ffmpeg} and file-downloading \cite{ftp} system, long-lived TCP
connection are also often maintained for fetching a large amount of data from servers. %These applications contribute to many long-lived flows.
 When NF instances handling long flows are overloaded, some flows need to be migrated to new NFs, in order to mitigate overload of the existing ones in a timely manner \cite{gember2015opennf}.
%Without flow migration, the overload of NF instances could not be mitigated in a timely manner because of long flows \cite{gember2015opennf}.

On the other hand, many NFs maintain important per-flow states. Intrusion
detection systems such as Bro \cite{bro} parse different network/application
protocols, and store and update protocol-related
states for each flow to alert potential attacks. Firewalls \cite{firewall}
maintain TCP connection-related states by parsing TCP SYN/ACK/FIN packets for
each flow. Some load-balancers \cite{lvs} use a map between flow identifiers and
the server address to modify the destination address in each flow packet.
It is critical to ensure correct recovery of flow states in case of NF instance failures, such that the connections handled by the failed NF instances do not have to be reset. In practice, middlebox vendors
strongly rejected the idea of simply resetting all active connections after failure as it
disrupts users \cite{sherry2015rollback}.

Given the importance of failure resilience and flow migration in an NFV system, why are they absent in the existing NF management systems? The reason is simple: implementing flow migration and fault
tolerance has been a challenging task on the existing NFV software architectures. %, as they do not provide a runtime environment with built-in resilience support.
 To provide resilience, important NF states must be correctly extracted from the NF software for transmitting to a new NF instance. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art implementation of NF software. Especially, important NF states may be scattered across the code base of the software, making
extracting and serializing NF states a daunting task. Patch codes need to be
manually added to the source code of different NFs to extract and serialize NF
states \cite{gember2015opennf}\cite{rajagopalan2013split}. This usually requires a huge amount of manual work to add up to
thousands of lines of source code for one NF, \eg, \cite{gember2015opennf} reports
that it needs to add 3.3K LOC for Bro \cite{bro} and 7.8K LOC for Squid caching
proxy \cite{squid}.  Realizing this difficulty, \cite{khalid2016paving} uses
static program analysis technique to automate this process. However, applying
static program analysis itself is a challenging task and the inaccuracy of
static program analysis may prevent some important NF states from being
correctly retrieved.

Even if NF states can be correctly acquired, transmitting
the states among different NFs %and the NFV system controller
 requires an
effective message passing service. The existing NF software (\eg,
Click\cite{kohler2000click}) does not usually provide the support for a messaging channel, and programmers have to manually add this communication
channel into the NF software. Finally, the additional codes that are patched to
implement resilience inevitably entangle with the core processing logic of NF
software. It may lead to serious software bugs if not handled properly.

In this paper, we propose a software framework for building resilient
NFV systems, \nfactor, exploiting the actor framework for programming distributed services \cite{actor-wiki, akka, newell2016optimizing} %\chuan{add more references here where actor framework is used to implement different services}.
Unlike existing work
\cite{gember2015opennf, sherry2015rollback} that patch resilience
functionalities into NF software, \nfactor~is an NFV
system with transparent resilience: (i) based on the actor programming model, a clean separation between important NF states and core NF processing logic is enabled in each NF module by a unique API, which makes extracting, serializing and transmitting important flow states an easy task; (ii) a new service chain abstraction enables running NF software modules belonging to the same service chain inside the
execution context of one actor, and the same runtime program is running on all containers containing multiple actors in the system %\chuan{point out how these designs are relevant to failure resilience}
;
(iii) a built-in efficient communication channel across the actors is available, in the actor framework.

%NFActor framework has the following unique features over existing NFV system.

%\textit{First}, NFActor framework executes the same runtime program on all the containers, instead of maintaining several containers, each running different kinds of NF softwares. On top of the uniform runtime program, NFActor framework builds a new NF abstraction, which runs NF software modules inside the execution context of an actor \cite{actor-wiki}. NFActor delegates the packet processing of a network flow to a unique actor and the actor constructs its own service chain to process the flow.

%Using NFActor, flows with different service chain requirements can pass through the same runtime container, increasing the resource utilization rate. It also greatly
%simplifies how data-plane paths of a NFV system are managed. There is no need to
%chain different NF instances on the data-plane as the service chain is
%dynamically created during actor execution.

%\textit{Second}, NFActor framework makes extracting, serializing and transmitting important flow states an easy task. This is because NFActor framework provides a unique API for creating new NF modules that enforces a clean separation between important NF states and core NF processing logic. And NFActor framework is built on top of actor programming model, which provides an efficient message passing channel.

%\textit{Third,} with the help of the first two features, NFActor framework becomes resilient by nature.

Our detailed contributions in designing \nfactor~can be summarized as follows. %\chuan{the paragraphs below on key contributions of the work need significant improvement}



{\em First}, we introduce the actor programming model into
NFV systems. NFV systems built on top of this model achieve transparent resilience and can exploit the efficient communication channel provided in the actor framework. Using \nfactor, programmer implementing NF modules only needs to focus on the core processing logic. The \nfactor~framework
automatically handles fault tolerance and flow migration for the created NF
modules. What's more, resilience support in \nfactor~is provided in a fully distributed fashion, without directly involving a central controller, which distinguishes \nfactor~from the existing NFV systems \cite{gember2015opennf}.

{\em Second}, we design an API for implementing each NF module, which enables a clean separation between important NF states and core NF processing logic. We propose a new service chain abstraction, where NFs in a service chain are deployed inside the execution context of
an actor, instead of being chained through different virtualized devices (\eg, virtual machines or containers). In this way, a unique actor is responsible for processing a network flow through a dedicated service chain. This unique actor fully monitors the flow processing. It can interrupt the flow processing for flow migration or fault tolerance without the need to contact the service chain.  %\chuan{explain benefits of doing this service chain abstraction}. \chuan{explain we choose to implement the system on containers and give why}.
We run the same runtime program on all containers containing multiple actors. The use of the container incurs only a small overhead and thereby improve the packet processing performance of NFActor framework. And using the same program ensures that a uniform execution environment is provided for all the actors, which facilitate the design of flow migration and fault tolerance.% which xxx \chuan{explain benefits of doing this}.

{\em Third}, we design a novel distributed flow migration method and a lightweight
flow state replication method, enabling fast and lightweight flow migration and failure recovery. The flow migration protocol used by NFActor framework only involves the transmission of 3 request-responses. Evaluation result shows that the migration of a single flow in an environment with small workload could be completed within 700us. The use of the actor abstraction enables NFActor framework to independently replicate the state of a single flow, thereby eliminating the need to halt the execution of the entire program. %\chuan{give highlights of your methods}

To enable a NF in \nfactor, core processing logic of an NF needs to be implemented following the actor programming framework. Nevertheless, porting the core
processing logic of an existing NF software should be relatively straightforward since the APIs provided by our \nfactor~framework is extremely simple. The evaluation result shows that NFActor only incurs acceptable overhead when running NF modules. And NFActor runtime systems have a promising linear scalability. For the resilience functionality, NFActor framework out-performs existing flow migration system by more than 50\% in terms of migration completion time and NFActor framework achieves consistent recovery time under varying workload. %\chuan{give evaluation results}

%Due to its unique API design, NF modules implemented in actors are not naturally compatible with legacy NF software. They must be manually ported to NFActor framework, which may also involve significant amount of labour work. However, porting the core processing logic of existing NF software should be relatively straight forward because the API provided by NFActor framework is extremely simple. And the primary focus of NFActor is to become a framework for implementing new NF softwares.

%Some evaluation result goes from here.

The rest of the paper is organized as follows. We present background about NFV and the actor model in Sec.~\ref{sec:background} and overview our \nfactor~system in Sec.~\ref{sec:overview}. We discuss in detail the fault tolerance and flow migration design in Sec.~\ref{sec:fm} and Sec.~\ref{sec:ft}. We show the implementation and evaluation results in Sec.~\ref{sec:implementation} and Sec.~\ref{sec:experiments}, followed by related work in Sec.~\ref{sec:relatedwork}. Sec.~\ref{sec:conclusion} concludes the paper.
